# CD Pipeline — Continuous Deployment (dev → staging → prod)
# Trigger: Artifact from CI pipeline on main branch
# Flow: Deploy to DEV (auto) → STAGING (auto + integration test) → PROD (manual approval)

trigger: none

resources:
  pipelines:
    - pipeline: ci-build
      source: "telco-churn-ci"
      trigger:
        branches:
          include:
            - main

pool:
  name: Default

variables:
  - name: CONDA_ACTIVATE
    value: "source ~/anaconda3/etc/profile.d/conda.sh && conda activate base"
  - name: DATA_FILES_PATH
    value: "/Users/kanaanudo/ado-agent/data"

stages:
  - stage: DeployDev
    displayName: "Deploy to DEV"
    variables:
      - group: databricks-dev
    jobs:
      - deployment: DeployToDev
        displayName: "Deploy to DEV workspace"
        environment: "telco-churn-dev"
        strategy:
          runOnce:
            deploy:
              steps:
                - download: ci-build
                  artifact: telco-churn-release

                - script: |
                    $(CONDA_ACTIVATE)
                    pip install databricks-cli databricks-sdk
                  displayName: "Install Databricks CLI"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=dev
                    export CATALOG_NAME=uk_telecoms_dev

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_notebooks.sh
                  displayName: "Deploy notebooks to DEV"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=dev

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_workflows.sh
                  displayName: "Deploy workflow to DEV"

  - stage: UploadData
    displayName: "Upload Data to Shared Landing"
    dependsOn: DeployDev
    jobs:
      - job: UploadData
        displayName: "Upload raw data to shared ADLS landing zone"
        steps:
          - download: ci-build
            artifact: telco-churn-release

          - script: |
              STORAGE_ACCOUNT="telcochurnsalanding"

              echo "Looking for data files in: $(DATA_FILES_PATH)"
              ls -lh $(DATA_FILES_PATH)/*.parquet $(DATA_FILES_PATH)/*.csv 2>/dev/null || true

              bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/upload_data.sh \
                "shared" "${STORAGE_ACCOUNT}" "$(DATA_FILES_PATH)"
            displayName: "Upload data files to shared landing storage"

  - stage: IntegrationTestDev
    displayName: "Integration Tests (DEV)"
    dependsOn: UploadData
    variables:
      - group: databricks-dev
    jobs:
      - job: RunIntegrationTests
        displayName: "Run Databricks integration tests"
        steps:
          - script: |
              $(CONDA_ACTIVATE)
              pip install databricks-sdk requests
            displayName: "Install SDK"

          - script: |
              $(CONDA_ACTIVATE)
              export DATABRICKS_HOST=$(DATABRICKS_HOST)
              export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)

              python $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/run_integration_tests.py
            displayName: "Run integration test notebook on DEV cluster"

  - stage: DeployStaging
    displayName: "Deploy to STAGING"
    dependsOn: IntegrationTestDev
    variables:
      - group: databricks-staging
    jobs:
      - deployment: DeployToStaging
        displayName: "Deploy to STAGING workspace"
        environment: "telco-churn-staging"
        strategy:
          runOnce:
            deploy:
              steps:
                - download: ci-build
                  artifact: telco-churn-release

                - script: |
                    $(CONDA_ACTIVATE)
                    pip install databricks-cli databricks-sdk
                  displayName: "Install Databricks CLI"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=staging
                    export CATALOG_NAME=uk_telecoms_staging

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_notebooks.sh
                  displayName: "Deploy notebooks to STAGING"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=staging

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_workflows.sh
                  displayName: "Deploy workflow to STAGING"

  - stage: DeployProd
    displayName: "Deploy to PRODUCTION"
    dependsOn: DeployStaging
    variables:
      - group: databricks-prod
    jobs:
      - deployment: DeployToProd
        displayName: "Deploy to PRODUCTION workspace"
        environment: "telco-churn-prod"
        strategy:
          runOnce:
            deploy:
              steps:
                - download: ci-build
                  artifact: telco-churn-release

                - script: |
                    $(CONDA_ACTIVATE)
                    pip install databricks-cli databricks-sdk
                  displayName: "Install Databricks CLI"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=prod
                    export CATALOG_NAME=uk_telecoms

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_notebooks.sh
                  displayName: "Deploy notebooks to PROD"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=prod

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_workflows.sh
                  displayName: "Deploy workflow to PROD"

                - script: |
                    echo "=== PRODUCTION DEPLOYMENT COMPLETE ==="
                    echo "Build: $(Build.BuildId)"
                    echo "Source: $(Build.SourceVersion)"
                    echo "Deployed at: $(date -u)"
                  displayName: "Deployment confirmation"

