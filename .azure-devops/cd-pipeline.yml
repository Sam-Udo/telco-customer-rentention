# CD Pipeline — Continuous Deployment (dev → staging → prod)
# Trigger: Artifact from CI pipeline on main branch
# Flow: Deploy to DEV (auto) → STAGING (auto + integration test) → PROD (manual approval)

trigger: none

resources:
  pipelines:
    - pipeline: ci-build
      source: "telco-churn-ci"
      trigger:
        branches:
          include:
            - main

pool:
  name: Default

variables:
  - name: CONDA_ACTIVATE
    value: "source ~/anaconda3/etc/profile.d/conda.sh && conda activate base"
  - name: DATA_FILES_PATH
    value: "/Users/kanaanudo/ado-agent/data"

stages:
  - stage: DeployDev
    displayName: "Deploy to DEV"
    variables:
      - group: databricks-dev
    jobs:
      - deployment: DeployToDev
        displayName: "Deploy to DEV workspace"
        environment: "telco-churn-dev"
        strategy:
          runOnce:
            deploy:
              steps:
                - download: ci-build
                  artifact: telco-churn-release

                - script: |
                    $(CONDA_ACTIVATE)
                    pip install databricks-cli databricks-sdk
                  displayName: "Install Databricks CLI"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=dev
                    export CATALOG_NAME=uk_telecoms_dev

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_notebooks.sh
                  displayName: "Deploy notebooks to DEV"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=dev

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_workflows.sh
                  displayName: "Deploy workflow to DEV"

  - stage: UploadData
    displayName: "Upload Data to Shared Landing"
    dependsOn: DeployDev
    jobs:
      - job: UploadData
        displayName: "Upload raw data to shared ADLS landing zone"
        steps:
          - download: ci-build
            artifact: telco-churn-release

          - script: |
              STORAGE_ACCOUNT="telcochurnsalanding"

              echo "Looking for data files in: $(DATA_FILES_PATH)"
              ls -lh $(DATA_FILES_PATH)/*.parquet $(DATA_FILES_PATH)/*.csv 2>/dev/null || true

              bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/upload_data.sh \
                "shared" "${STORAGE_ACCOUNT}" "$(DATA_FILES_PATH)"
            displayName: "Upload data files to shared landing storage"

  - stage: IntegrationTestDev
    displayName: "Integration Tests (DEV)"
    dependsOn: UploadData
    variables:
      - group: databricks-dev
    jobs:
      - job: RunIntegrationTests
        displayName: "Run Databricks integration tests"
        steps:
          - script: |
              $(CONDA_ACTIVATE)
              pip install databricks-sdk requests
            displayName: "Install SDK"

          - script: |
              $(CONDA_ACTIVATE)
              export DATABRICKS_HOST=$(DATABRICKS_HOST)
              export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)

              python $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/run_integration_tests.py
            displayName: "Run integration test notebook on DEV cluster"

  - stage: SyncModelsToAKS
    displayName: "Sync Models to AKS (DEV)"
    dependsOn: IntegrationTestDev
    variables:
      - group: databricks-dev
      - group: aks-dev
    jobs:
      - job: ExportAndSync
        displayName: "Export models from Databricks → Sync to AKS"
        steps:
          - download: ci-build
            artifact: telco-churn-release

          - script: |
              $(CONDA_ACTIVATE)
              pip install requests
            displayName: "Install dependencies"

          - script: |
              $(CONDA_ACTIVATE)
              export DATABRICKS_HOST=$(DATABRICKS_HOST)
              export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)

              echo "Triggering model export on Databricks..."
              RESULT=$(python3 -c "
import requests, json
host = '$(DATABRICKS_HOST)'.rstrip('/')
headers = {'Authorization': 'Bearer $(DATABRICKS_TOKEN)', 'Content-Type': 'application/json'}
payload = {
    'run_name': 'export_models_for_aks_cd',
    'tasks': [{
        'task_key': 'export',
        'notebook_task': {
            'notebook_path': '/Workspace/telco-churn-dev/notebooks/07_export_models',
            'source': 'WORKSPACE'
        },
        'new_cluster': {
            'spark_version': '14.3.x-scala2.12',
            'node_type_id': 'Standard_D4pds_v6',
            'num_workers': 0,
            'data_security_mode': 'SINGLE_USER',
            'spark_conf': {
                'spark.master': 'local[*]',
                'spark.databricks.cluster.profile': 'singleNode'
            },
            'custom_tags': {'ResourceClass': 'SingleNode'}
        },
        'libraries': [
            {'pypi': {'package': 'mlflow[databricks]'}},
            {'pypi': {'package': 'lightgbm'}}
        ]
    }]
}
resp = requests.post(f'{host}/api/2.1/jobs/runs/submit', headers=headers, json=payload)
data = resp.json()
print(data.get('run_id', ''))
")
              echo "Export run ID: ${RESULT}"

              echo "Waiting for export to complete..."
              python3 -c "
import requests, time, sys
host = '$(DATABRICKS_HOST)'.rstrip('/')
headers = {'Authorization': 'Bearer $(DATABRICKS_TOKEN)'}
run_id = '${RESULT}'
for attempt in range(60):  # 10 minutes max
    resp = requests.get(f'{host}/api/2.1/jobs/runs/get?run_id={run_id}', headers=headers)
    state = resp.json().get('state', {})
    lifecycle = state.get('life_cycle_state', 'UNKNOWN')
    result = state.get('result_state', '')
    print(f'  [{attempt+1}/60] {lifecycle} / {result}')
    if lifecycle == 'TERMINATED':
        if result == 'SUCCESS':
            print('Export succeeded')
            sys.exit(0)
        else:
            print(f'Export failed: {result}')
            sys.exit(1)
    if lifecycle == 'INTERNAL_ERROR':
        print(f'Export failed: {lifecycle}')
        sys.exit(1)
    time.sleep(10)
print('Timeout waiting for export')
sys.exit(1)
"
            displayName: "Run model export on Databricks"

          - script: |
              export DATABRICKS_HOST=$(DATABRICKS_HOST)
              export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
              export AKS_RESOURCE_GROUP=$(AKS_RESOURCE_GROUP)
              export AKS_CLUSTER_NAME=$(AKS_CLUSTER_NAME)
              export TARGET_ENV=dev

              bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/sync_models_to_aks.sh
            displayName: "Sync models to AKS DEV"

  - stage: DeployStaging
    displayName: "Deploy to STAGING"
    dependsOn: SyncModelsToAKS
    variables:
      - group: databricks-staging
    jobs:
      - deployment: DeployToStaging
        displayName: "Deploy to STAGING workspace"
        environment: "telco-churn-staging"
        strategy:
          runOnce:
            deploy:
              steps:
                - download: ci-build
                  artifact: telco-churn-release

                - script: |
                    $(CONDA_ACTIVATE)
                    pip install databricks-cli databricks-sdk
                  displayName: "Install Databricks CLI"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=staging
                    export CATALOG_NAME=uk_telecoms_staging

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_notebooks.sh
                  displayName: "Deploy notebooks to STAGING"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=staging

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_workflows.sh
                  displayName: "Deploy workflow to STAGING"

  - stage: DeployProd
    displayName: "Deploy to PRODUCTION"
    dependsOn: DeployStaging
    variables:
      - group: databricks-prod
    jobs:
      - deployment: DeployToProd
        displayName: "Deploy to PRODUCTION workspace"
        environment: "telco-churn-prod"
        strategy:
          runOnce:
            deploy:
              steps:
                - download: ci-build
                  artifact: telco-churn-release

                - script: |
                    $(CONDA_ACTIVATE)
                    pip install databricks-cli databricks-sdk
                  displayName: "Install Databricks CLI"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=prod
                    export CATALOG_NAME=uk_telecoms

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_notebooks.sh
                  displayName: "Deploy notebooks to PROD"

                - script: |
                    $(CONDA_ACTIVATE)
                    export DATABRICKS_HOST=$(DATABRICKS_HOST)
                    export DATABRICKS_TOKEN=$(DATABRICKS_TOKEN)
                    export TARGET_ENV=prod

                    bash $(Pipeline.Workspace)/ci-build/telco-churn-release/deploy/deploy_workflows.sh
                  displayName: "Deploy workflow to PROD"

                - script: |
                    echo "=== PRODUCTION DEPLOYMENT COMPLETE ==="
                    echo "Build: $(Build.BuildId)"
                    echo "Source: $(Build.SourceVersion)"
                    echo "Deployed at: $(date -u)"
                  displayName: "Deployment confirmation"

